{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## What is Explainable AI (XAI)?\n",
        "Explainable AI refers to a set of processes and methods that aim to provide a clear and human-understandable explanation for the decisions generated by AI and machine learning models.\n",
        "\n",
        "Integrating an explainability layer into these models, Data Scientists and Machine Learning practitioners can create more trustworthy and transparent systems to assist a wide range of stakeholders such as developers, regulators, and end-users.\n",
        "\n",
        "## Building Trust Through Explainable AI\n",
        "Here are some explainable AI principles that can contribute to building trust:\n",
        "\n",
        "* Transparency:\n",
        " Ensuring stakeholders understand the models’ decision-making process.\n",
        "* Fairness:\n",
        " Ensuring that the models’ decisions are fair for everyone, including people in protected groups (race, religion, gender, disability, ethnicity).\n",
        "* Trust:\n",
        " Assessing the confidence level of human users using the AI system.\n",
        "Robustness. Being resilient to changes in input data or model parameters, maintaining consistent and reliable performance even when faced with uncertainty or unexpected situations.\n",
        "* Privacy:\n",
        " Guaranteeing the protection of sensitive user information.\n",
        "* Interpretability:\n",
        " Providing human-understandable explanations for their predictions and outcomes.\n",
        "\n",
        "\n",
        "## Build Classifier\n",
        "A Random Forest classifier is built to predict diabetes outcomes using the diabetes dataset. The code is broken down into several steps: (1) import relevant libraries, (2) create training and testing datasets, (3) build the model, and (4) report the performance metrics through the classification report."
      ],
      "metadata": {
        "id": "hl2HpeomItLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYbhPBI-ImIY"
      },
      "outputs": [],
      "source": [
        "# Load useful libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Separate Features and Target Variables\n",
        "X = diabetes_data.drop(columns='Outcome')\n",
        "y = diabetes_data['Outcome']\n",
        "\n",
        "# Create Train & Test Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,\n",
        "                                                \tstratify =y,\n",
        "                                                \trandom_state = 13)\n",
        "\n",
        "# Build the model\n",
        "rf_clf = RandomForestClassifier(max_features=2, n_estimators =100 ,bootstrap = True)\n",
        "\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction on the testing data\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-agnostic methods\n",
        "These methods can be applied to any machine learning model, regardless of its structure or type. They focus on analyzing the features’ input-output pair. This section will introduce and discuss LIME and SHAP, two widely-used surrogate models.\n",
        "\n",
        "# SHAP\n",
        "It stands for SHapley Additive exPlanations. This method aims to explain the prediction of an instance/observation by computing the contribution of each feature to the prediction, and it can be installed using the following pip command."
      ],
      "metadata": {
        "id": "mkf3qeWOJXtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "ZliF0ll_JdtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After the installation:\n",
        "\n",
        "* The main shap library is imported.\n",
        "* The TreeExplainer class is used to explain tree-based models, along with the initjs.\n",
        "* shape.initjs() function initializes the JavaScript code required to display SHAP visualizations in a jupyter notebook environment.\n",
        "* Finally, after instantiating the TreeExplainer class with the random forest classifier, the shape values are computed for each feature of each instance in the test dataset.\n"
      ],
      "metadata": {
        "id": "FBGg9sMTJgu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load JS visualization code to notebook\n",
        "shap.initjs()\n",
        "\n",
        "# Create the explainer\n",
        "explainer = shap.TreeExplainer(rf_clf)\n",
        "\n",
        "shap_values = explainer.shap_values(X_test)"
      ],
      "metadata": {
        "id": "cDzivjZbJvKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP offers an array of visualization tools for enhancing model interpretability, and the next section will discuss two of them: (1) variable importance with the summary plot, (2) summary plot of a specific target, and (3) dependence plot.\n",
        "\n",
        "## Variable Importance with Summary Plot\n",
        "In this plot, features are ranked by their average SHAP values showing the most important features at the top and the least important ones at the bottom using the summary_plot() function. This helps to understand the impact of each feature on the model’s predictions."
      ],
      "metadata": {
        "id": "Z2BPZkTkJyfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Variable Importance Plot - Global Interpretation\")\n",
        "figure = plt.figure()\n",
        "shap.summary_plot(shap_values, X_test)"
      ],
      "metadata": {
        "id": "QSTMfekfJ6io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the interpretation that can be made from the above graphic:\n",
        "\n",
        "* We can observe that the red and blue colors occupy half of the horizontal rectangles for each class. This means that each feature has an equal impact on the classification of both diabetes (label=1) and non-diabetes (label=0) cases.\n",
        "* However, Glucose, Age, and BMI are the first three features with the most predictive power.\n",
        "* On the other hand, Pregnancies, SkinThicknes, Insulin, and BloodPressure do not contribute as much as the first three features.\n"
      ],
      "metadata": {
        "id": "zqGSDSZUJ-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LIME\n",
        "Local Interpretable Model-agnostic Explanations (LIME for short). Instead of providing a global understanding of the model on the entire dataset, LIME focuses on explaining the model’s prediction for individual instances.\n",
        "\n",
        "LIME explainer can be set up using two main steps: (1) import the lime module, and (2) fit the explainer using the training data and the targets. During this phase, the mode is set to classification, which corresponds to the task being performed."
      ],
      "metadata": {
        "id": "3Ozld-oQKQhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the LimeTabularExplainer module\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "# Get the class names\n",
        "class_names = ['Has diabetes', 'No diabetes']\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = list(X_train.columns)\n",
        "\n",
        "# Fit the Explainer on the training data set using the LimeTabularExplainer\n",
        "explainer = LimeTabularExplainer(X_train.values, feature_names =\n",
        "                                 feature_names,\n",
        "                                 class_names = class_names,\n",
        "                                 mode = 'classification')"
      ],
      "metadata": {
        "id": "Z2F9qHM9J9Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result contains three main pieces of information from left to right: (1) the model’s predictions, (2) features contributions, and (3) the actual value for each feature.\n",
        "\n",
        "We can observe that the eight patient is predicted to have diabetes with 72% confidence. The reasons that led the model to make this decision is because:\n",
        "\n",
        "* The patient’s glucose level is more than 99.\n",
        "* The blood pressure is more than 70.\n",
        "Those values can be verified from the table on the right.\n",
        "\n",
        "## Model-specific methods\n",
        "As opposed to model-agnostic methods, these methods can only be applied to a limited category of models. Some of those models include linear regression, decision trees, and neural network interpretability. Different technics such as DeepLIFT, Grad-CAM, or Integrated Gradients can be leveraged to explain deep-learning models.\n",
        "\n",
        "When using a decision-tree model, a graphical tree can be generated with the plot_tree function from scikit-learn to explain the decision-making process of the model from top-to-bottom, and an illustration is given below.\n",
        "\n",
        "The Deep Learning - A Tutorial for Data Scientists article will answer the most frequently asked questions about deep learning and explores various aspects of deep learning with real-life examples.\n",
        "\n",
        "Let's train a decision tree classifier with specific hyperparameters like max_depth, and min_samples_leaf before generating the graphical tree."
      ],
      "metadata": {
        "id": "6x3aPJemKdeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "dt_clf = DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 2)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data and evaluate the model\n",
        "y_pred = dt_clf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "jOw_HXJCKlOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges of XAI and Future Perspectives\n",
        "As AI technology continues to advance and become more sophisticated, understanding and interpreting the algorithms to discern how they produce outcomes is becoming increasingly challenging, allowing researchers to continue exploring new approaches and improving existing ones.\n",
        "\n",
        "Many explainable AI models require simplifying the underlying model, leading to a loss of predictive performance. In addition, current explainability methods may not cover all the aspects of the decision-making process, which can limit the benefit of the explanation, especially when dealing with more complex models.\n",
        "\n",
        "New research methods are focusing on enhancing explainable AI technics by developing more effective algorithms to address ethical issues while creating user-friendly explanations.\n",
        "\n",
        "Finally, with the ongoing research, we are more likely to have more sophisticated methods that promote transparency, trustworthiness, and fairness.\n",
        "\n",
        "## Conclusion\n",
        "This article has provided a good overview of what explainable AI is and some principles that contribute to building trust and can provide Data Scientists and other stakeholders with relevant skillsets to build trustworthy models to help make actionable decisions.\n",
        "\n",
        "We also covered model-agnostic and model-specific methods with a special focus on the first one using LIME and SHAP. Furthermore, the challenges, limitations, and some areas of research have been highlighted about explainable AI.\n",
        "\n",
        "To learn more about the ethics behind such technology, check out our Introduction to Data Ethics course, which covers principles of data ethics, its relationship with AI ethics, and its characteristics across the different stages of the data lifecycle."
      ],
      "metadata": {
        "id": "5n0Gw8BQKkjZ"
      }
    }
  ]
}